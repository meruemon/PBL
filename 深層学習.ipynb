{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOpenCV、Dlib、MediaPipeを用いた画像認識 ｰ深層学習ｰ\n",
    "\n",
    "ここでは、カメラから取得した映像を用いて画像認識を行い、\n",
    "必要な情報を取得するための流れを学ぶことで、\n",
    "画像認識をビジネス現場で応用するイメージをつかみます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MediaPipeを用いた骨格検出\n",
    "\n",
    "このPythonスクリプトは、OpenCVとOpenPoseを使用して、リアルタイムの人体ポーズ推定を行います。主な機能は以下の通りです：\n",
    "\n",
    "1. ウェブカメラからのビデオ入力を取得します。\n",
    "2. MediaPipeのPoseソリューションを使用して、人体の33個の主要な関節点（ランドマーク）を検出します。\n",
    "3. 検出された骨格を可視化し、関節間の接続線を描画します。\n",
    "4. 肘と膝の関節角度を計算して表示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 53\u001b[0m\n\u001b[0;32m     50\u001b[0m image_rgb\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# ポーズ検出を実行\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mpose\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_rgb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# 画像を書き込み可能に戻し、RGBからBGRに変換\u001b[39;00m\n\u001b[0;32m     56\u001b[0m image_rgb\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\app\\anaconda3\\envs\\pbl_cv47\\lib\\site-packages\\mediapipe\\python\\solutions\\pose.py:185\u001b[0m, in \u001b[0;36mPose.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[0;32m    165\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the pose landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m landmark \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks\u001b[38;5;241m.\u001b[39mlandmark:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n",
      "File \u001b[1;32mc:\\app\\anaconda3\\envs\\pbl_cv47\\lib\\site-packages\\mediapipe\\python\\solution_base.py:335\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    329\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    331\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[0;32m    332\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    333\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m--> 335\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[0;32m    338\u001b[0m solution_outputs \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mnamedtuple(\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSolutionOutputs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info\u001b[38;5;241m.\u001b[39mkeys())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# MediaPipeのポーズ検出モジュールを初期化\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# ポーズ検出器を設定（静的画像モードはFalse、最大1人検出）\n",
    "pose = mp_pose.Pose(\n",
    "    static_image_mode=False,\n",
    "    model_complexity=1,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "def calculate_angle(a, b, c):\n",
    "    \"\"\"\n",
    "    3つの点から角度を計算する関数\n",
    "    a: 始点の座標 [x, y]\n",
    "    b: 中心点の座標 [x, y]（角度を測る点）\n",
    "    c: 終点の座標 [x, y]\n",
    "    戻り値: 角度（度数法）\n",
    "    \"\"\"\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    c = np.array(c)\n",
    "    \n",
    "    # ベクトルBA、BCを計算\n",
    "    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "    angle = np.abs(radians * 180.0 / np.pi)\n",
    "    \n",
    "    # 180度を超える場合は、小さい方の角度を採用\n",
    "    if angle > 180.0:\n",
    "        angle = 360 - angle\n",
    "        \n",
    "    return angle\n",
    "\n",
    "# Webカメラを起動（0は既定のカメラ）\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"カメラから映像を取得できません\")\n",
    "        break\n",
    "    \n",
    "    # BGRからRGBに変換（MediaPipeはRGB形式を使用）\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image_rgb.flags.writeable = False\n",
    "    \n",
    "    # ポーズ検出を実行\n",
    "    results = pose.process(image_rgb)\n",
    "    \n",
    "    # 画像を書き込み可能に戻し、RGBからBGRに変換\n",
    "    image_rgb.flags.writeable = True\n",
    "    image = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # ランドマークが検出された場合\n",
    "    if results.pose_landmarks:\n",
    "        # 骨格を描画\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image,\n",
    "            results.pose_landmarks,\n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2, circle_radius=2),\n",
    "            mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2)\n",
    "        )\n",
    "        \n",
    "        # 画像のサイズを取得\n",
    "        height, width, _ = image.shape\n",
    "        \n",
    "        # ランドマークを取得\n",
    "        landmarks = results.pose_landmarks.landmark\n",
    "        \n",
    "        # 左肘の角度を計算（肩-肘-手首）\n",
    "        try:\n",
    "            # 左肩、左肘、左手首の座標を取得\n",
    "            left_shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER].x * width,\n",
    "                           landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER].y * height]\n",
    "            left_elbow = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW].x * width,\n",
    "                         landmarks[mp_pose.PoseLandmark.LEFT_ELBOW].y * height]\n",
    "            left_wrist = [landmarks[mp_pose.PoseLandmark.LEFT_WRIST].x * width,\n",
    "                         landmarks[mp_pose.PoseLandmark.LEFT_WRIST].y * height]\n",
    "            \n",
    "            # 角度を計算\n",
    "            left_elbow_angle = calculate_angle(left_shoulder, left_elbow, left_wrist)\n",
    "            \n",
    "            # 角度を画面に表示\n",
    "            cv2.putText(image, f\"Left Elbow: {int(left_elbow_angle)} deg\",\n",
    "                       (int(left_elbow[0]), int(left_elbow[1])),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # 右肘の角度を計算（肩-肘-手首）\n",
    "        try:\n",
    "            # 右肩、右肘、右手首の座標を取得\n",
    "            right_shoulder = [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER].x * width,\n",
    "                            landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER].y * height]\n",
    "            right_elbow = [landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW].x * width,\n",
    "                          landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW].y * height]\n",
    "            right_wrist = [landmarks[mp_pose.PoseLandmark.RIGHT_WRIST].x * width,\n",
    "                          landmarks[mp_pose.PoseLandmark.RIGHT_WRIST].y * height]\n",
    "            \n",
    "            # 角度を計算\n",
    "            right_elbow_angle = calculate_angle(right_shoulder, right_elbow, right_wrist)\n",
    "            \n",
    "            # 角度を画面に表示\n",
    "            cv2.putText(image, f\"Right Elbow: {int(right_elbow_angle)} deg\",\n",
    "                       (int(right_elbow[0]), int(right_elbow[1])),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # 左膝の角度を計算（腰-膝-足首）\n",
    "        try:\n",
    "            # 左腰、左膝、左足首の座標を取得\n",
    "            left_hip = [landmarks[mp_pose.PoseLandmark.LEFT_HIP].x * width,\n",
    "                       landmarks[mp_pose.PoseLandmark.LEFT_HIP].y * height]\n",
    "            left_knee = [landmarks[mp_pose.PoseLandmark.LEFT_KNEE].x * width,\n",
    "                        landmarks[mp_pose.PoseLandmark.LEFT_KNEE].y * height]\n",
    "            left_ankle = [landmarks[mp_pose.PoseLandmark.LEFT_ANKLE].x * width,\n",
    "                         landmarks[mp_pose.PoseLandmark.LEFT_ANKLE].y * height]\n",
    "            \n",
    "            # 角度を計算\n",
    "            left_knee_angle = calculate_angle(left_hip, left_knee, left_ankle)\n",
    "            \n",
    "            # 角度を画面に表示\n",
    "            cv2.putText(image, f\"Left Knee: {int(left_knee_angle)} deg\",\n",
    "                       (int(left_knee[0]), int(left_knee[1])),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2, cv2.LINE_AA)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # 右膝の角度を計算（腰-膝-足首）\n",
    "        try:\n",
    "            # 右腰、右膝、右足首の座標を取得\n",
    "            right_hip = [landmarks[mp_pose.PoseLandmark.RIGHT_HIP].x * width,\n",
    "                        landmarks[mp_pose.PoseLandmark.RIGHT_HIP].y * height]\n",
    "            right_knee = [landmarks[mp_pose.PoseLandmark.RIGHT_KNEE].x * width,\n",
    "                         landmarks[mp_pose.PoseLandmark.RIGHT_KNEE].y * height]\n",
    "            right_ankle = [landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE].x * width,\n",
    "                          landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE].y * height]\n",
    "            \n",
    "            # 角度を計算\n",
    "            right_knee_angle = calculate_angle(right_hip, right_knee, right_ankle)\n",
    "            \n",
    "            # 角度を画面に表示\n",
    "            cv2.putText(image, f\"Right Knee: {int(right_knee_angle)} deg\",\n",
    "                       (int(right_knee[0]), int(right_knee[1])),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2, cv2.LINE_AA)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # 操作説明を表示\n",
    "    cv2.putText(image, \"Press 'q' to quit\", (10, 30),\n",
    "               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "    \n",
    "    # 結果を表示\n",
    "    cv2.imshow('MediaPipe Pose Detection', image)\n",
    "    \n",
    "    # 'q'キーで終了\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# リソースを解放\n",
    "pose.close()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Darknet (Yolov5)\n",
    "\n",
    "YOLOv5は、Ultralytics社によって開発された、YOLOv3を改良したリアルタイム物体検出アルゴリズムです。高速かつ高精度な検出が可能です。軽量なモデルから高精度なモデルまで、サイズの異なる複数のモデルが提供されており、簡単にカスタマイズや学習ができるため、様々な用途に適しています。YOLOv5に関する情報は豊富で、自作の物体検出器を構築する際には、v5の利用をおすすめします。\n",
    "\n",
    "**本プログラムを実行する際には、OpenCVのバージョンアップデートが必要なため、GitHub上の講義資料に従って専用の環境構築を行ってください。**\n",
    "\n",
    "このプログラムはOpenCVの[参考資料](https://learnopencv.com/object-detection-using-yolov5-and-opencv-dnn-in-c-and-python/)を元に作成されています．\n",
    "\n",
    "このプログラムを実行するためには、学習済みのDNNモデルが必要です。Google Colab上で以下のコードを順に実行して、`yolov5s.onnx`を作成してダウンロードしてください。または、[ここから](https://kutc-my.sharepoint.com/:u:/g/personal/t160024_kansai-u_ac_jp/ETb0zCwQVPlPo5-dPP1HitIB1erpb53UGZzaZkHQgyQuhA?e=CAGSPG)ダウンロードしてください。保存先は、フォルダ`det/`です。\n",
    "\n",
    "```\n",
    "!git clone https://github.com/ultralytics/yolov5\n",
    "%cd /content/yolov5/\n",
    "!pip install -r requirements.txt\n",
    "!wget https://github.com/ultralytics/YOLOv5/releases/download/v6.1/YOLOv5s.pt\n",
    "!python export.py --weights YOLOv5s.pt --opset 12 --include onnx\n",
    "```\n",
    "\n",
    "その他の参考資料\n",
    "\n",
    "[https://github.com/ultralytics/yolov5?tab=readme-ov-file](https://github.com/ultralytics/yolov5?tab=readme-ov-file)\n",
    "\n",
    "[https://docs.ultralytics.com/ja](https://docs.ultralytics.com/ja)\n",
    "\n",
    "[https://docs.ultralytics.com/ja/yolov5/tutorials/model_export/](https://docs.ultralytics.com/ja/yolov5/tutorials/model_export/)\n",
    "\n",
    "[https://qiita.com/02130515/items/cfbb9dcd8291418476ab](https://qiita.com/02130515/items/cfbb9dcd8291418476ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# 定数設定\n",
    "INPUT_WIDTH = 640  # 入力画像の幅\n",
    "INPUT_HEIGHT = 640  # 入力画像の高さ\n",
    "SCORE_THRESHOLD = 0.5  # スコアのしきい値\n",
    "NMS_THRESHOLD = 0.45  # 非最大抑制のしきい値\n",
    "CONFIDENCE_THRESHOLD = 0.45  # 信頼度のしきい値\n",
    "\n",
    "# テキスト表示に関する設定\n",
    "FONT_FACE = cv2.FONT_HERSHEY_SIMPLEX\n",
    "FONT_SCALE = 0.7  # フォントのサイズ\n",
    "THICKNESS = 1  # テキストの太さ\n",
    "\n",
    "# 色の設定\n",
    "BLACK  = (0, 0, 0)\n",
    "BLUE   = (255, 178, 50)\n",
    "YELLOW = (0, 255, 255)\n",
    "RED = (0, 0, 255)\n",
    "\n",
    "# ラベルを描画する関数\n",
    "def draw_label(input_image, label, left, top):\n",
    "    \"\"\"ラベル（テキスト）を指定の位置に描画する\"\"\"\n",
    "    \n",
    "    # テキストのサイズを取得\n",
    "    text_size = cv2.getTextSize(label, FONT_FACE, FONT_SCALE, THICKNESS)\n",
    "    dim, baseline = text_size[0], text_size[1]\n",
    "    # テキストの背景用の黒い矩形を描画\n",
    "    cv2.rectangle(input_image, (left, top), (left + dim[0], top + dim[1] + baseline), BLACK, cv2.FILLED)\n",
    "    # 矩形の中にテキストを描画\n",
    "    cv2.putText(input_image, label, (left, top + dim[1]), FONT_FACE, FONT_SCALE, YELLOW, THICKNESS, cv2.LINE_AA)\n",
    "\n",
    "# 前処理を行う関数\n",
    "def pre_process(input_image, net):\n",
    "    \"\"\"画像をネットワークに入力するための前処理\"\"\"\n",
    "    \n",
    "    # 画像をBLOB形式に変換\n",
    "    blob = cv2.dnn.blobFromImage(input_image, 1/255, (INPUT_WIDTH, INPUT_HEIGHT), [0,0,0], 1, crop=False)\n",
    "\n",
    "    # ネットワークに入力をセット\n",
    "    net.setInput(blob)\n",
    "\n",
    "    # 出力レイヤーの名前を取得し、フォワードパスで結果を取得\n",
    "    output_layers = net.getUnconnectedOutLayersNames()\n",
    "    outputs = net.forward(output_layers)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "# 後処理を行う関数\n",
    "def post_process(input_image, outputs, classes):\n",
    "    \"\"\"ネットワークの出力結果に基づいて後処理を行い、物体を検出して描画する\"\"\"\n",
    "    \n",
    "    # クラスID、信頼度、バウンディングボックスを保存するリスト\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    # 出力結果の行数を取得\n",
    "    rows = outputs[0].shape[1]\n",
    "\n",
    "    # 入力画像のサイズを取得\n",
    "    image_height, image_width = input_image.shape[:2]\n",
    "\n",
    "    # リサイズ係数の計算\n",
    "    x_factor = image_width / INPUT_WIDTH\n",
    "    y_factor = image_height / INPUT_HEIGHT\n",
    "\n",
    "    # 各検出結果を処理\n",
    "    for r in range(rows):\n",
    "        row = outputs[0][0][r]\n",
    "        confidence = row[4]  # 信頼度を取得\n",
    "\n",
    "        # 信頼度が閾値を超えた場合のみ処理\n",
    "        if confidence >= CONFIDENCE_THRESHOLD:\n",
    "            classes_scores = row[5:]  # 各クラスのスコアを取得\n",
    "\n",
    "            # スコアが最も高いクラスIDを取得\n",
    "            class_id = np.argmax(classes_scores)\n",
    "\n",
    "            # スコアがしきい値を超える場合のみ処理\n",
    "            if (classes_scores[class_id] > SCORE_THRESHOLD):\n",
    "                confidences.append(confidence)\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "                # バウンディングボックスの中心座標、幅、高さを取得\n",
    "                cx, cy, w, h = row[0], row[1], row[2], row[3]\n",
    "\n",
    "                # 左上の座標を計算\n",
    "                left = int((cx - w/2) * x_factor)\n",
    "                top = int((cy - h/2) * y_factor)\n",
    "                width = int(w * x_factor)\n",
    "                height = int(h * y_factor)\n",
    "\n",
    "                # バウンディングボックスを保存\n",
    "                box = np.array([left, top, width, height])\n",
    "                boxes.append(box)\n",
    "\n",
    "    # 非最大抑制を使用して重複するボックスを除去\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, CONFIDENCE_THRESHOLD, NMS_THRESHOLD)\n",
    "    for i in indices:\n",
    "        box = boxes[i]\n",
    "        left = box[0]\n",
    "        top = box[1]\n",
    "        width = box[2]\n",
    "        height = box[3]\n",
    "        # バウンディングボックスを描画\n",
    "        cv2.rectangle(input_image, (left, top), (left + width, top + height), BLUE, 3*THICKNESS)\n",
    "        # ラベルを描画\n",
    "        label = \"{}:{:.2f}\".format(classes[class_ids[i]], confidences[i])\n",
    "        draw_label(input_image, label, left, top)\n",
    "\n",
    "    return input_image\n",
    "\n",
    "# カメラ映像を使ったリアルタイム物体検出\n",
    "def run_real_time_detection():\n",
    "    \"\"\"カメラを使用してリアルタイムで物体検出を行う\"\"\"\n",
    "\n",
    "    # クラス名を読み込み\n",
    "    classesFile = \"det/coco.names\"\n",
    "    classes = None\n",
    "    with open(classesFile, 'rt') as f:\n",
    "        classes = f.read().rstrip('\\n').split('\\n')\n",
    "\n",
    "    # YOLOv5モデルを読み込む\n",
    "    modelWeights = \"det/yolov5s.onnx\"\n",
    "    net = cv2.dnn.readNet(modelWeights)\n",
    "\n",
    "    # デフォルトカメラ（0）を使用して映像を取得\n",
    "    cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "\n",
    "    while True:\n",
    "        # カメラからフレームをキャプチャ\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"カメラからフレームを取得できませんでした。\")\n",
    "            break\n",
    "\n",
    "        # 前処理を行い、検出を実行\n",
    "        detections = pre_process(frame, net)\n",
    "\n",
    "        # 検出結果に基づいて後処理を行い、フレームに描画\n",
    "        processed_frame = post_process(frame.copy(), detections, classes)\n",
    "\n",
    "        # 検出結果を別ウィンドウで表示\n",
    "        cv2.imshow('YOLOv5 Object Detection', processed_frame)\n",
    "\n",
    "        # 'q'キーを押すと終了\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # 終了時にカメラを解放し、ウィンドウを閉じる\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# リアルタイム物体検出を実行\n",
    "run_real_time_detection()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pbl_cv47",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
