{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenCVとDlibを用いた画像認識 ｰ深層学習ｰ\n",
    "\n",
    "ここでは、カメラから取得した映像を用いて画像認識を行い、\n",
    "必要な情報を取得するための流れを学ぶことで、\n",
    "画像認識をビジネス現場で応用するイメージをつかみます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【上級】MobileNetSSDを用いた人の検出\n",
    "\n",
    "OpenCVとMobileNet SSDモデルを使用して、ビデオ内の人物を検出するプログラムです。主な機能は以下の通りです：\n",
    "\n",
    "1. 事前学習済みのMobileNet SSDモデルを読み込みます。\n",
    "2. 指定されたビデオファイルを開き、フレームごとに処理を行います。\n",
    "3. 各フレームに対して、MobileNet SSDモデルを使用して物体検出を実行します。\n",
    "4. 検出された物体の中から「人」のみを抽出し、バウンディングボックスと信頼度を表示します。\n",
    "5. 処理結果をリアルタイムで画面に表示します。\n",
    "6. Qキーが押されるまで処理を続け、終了時にはリソースを解放します。\n",
    "\n",
    "MobileNetSSDのは、次のページをしてください。\n",
    "\n",
    "[MobileNetSSD](Dhttps://github.com/chuanqi305/MobileNet-SSD/tree/master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# MobileNet SSDモデルの設定ファイルのパスを指定\n",
    "prototxt_path = 'det/MobileNetSSD_deploy.prototxt'\n",
    "model_path = 'det/MobileNetSSD_deploy.caffemodel'\n",
    "\n",
    "# MobileNet SSDが検出できるオブジェクトクラスのリスト\n",
    "CLASSES = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\",\n",
    "           \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\",\n",
    "           \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\",\n",
    "           \"sofa\", \"train\", \"tvmonitor\"]\n",
    "\n",
    "# 事前学習済みのMobileNet SSDモデルを読み込む\n",
    "net = cv2.dnn.readNetFromCaffe(prototxt_path, model_path)\n",
    "\n",
    "# ビデオファイルを開く\n",
    "cap = cv2.VideoCapture('vtest.avi')\n",
    "\n",
    "# メインループ：フレームごとに処理を行う\n",
    "while True:\n",
    "    # ビデオからフレームを1枚読み込む\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break  # ビデオの終わりに達したらループを抜ける\n",
    "\n",
    "    # フレームの高さと幅を取得\n",
    "    (h, w) = frame.shape[:2]\n",
    "\n",
    "    # 入力画像を前処理し、ニューラルネットワークに入力できる形式に変換\n",
    "    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)),\n",
    "                                 0.007843, (300, 300), 127.5)\n",
    "\n",
    "    # ニューラルネットワークに入力データをセットし、順伝播を実行\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "\n",
    "    # 検出結果をループで処理\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        # 信頼度が0.2より高い検出結果のみを処理\n",
    "        if confidence > 0.2:\n",
    "            idx = int(detections[0, 0, i, 1])\n",
    "            # 検出されたオブジェクトが「人」の場合のみ処理\n",
    "            if CLASSES[idx] == \"person\":\n",
    "                # バウンディングボックスの座標を計算\n",
    "                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "                (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "                # フレーム上にバウンディングボックスとラベルを描画\n",
    "                label = f\"Person: {confidence:.2f}\"\n",
    "                cv2.rectangle(frame, (startX, startY), (endX, endY),\n",
    "                              (0, 255, 0), 2)\n",
    "                y = startY - 15 if startY - 15 > 15 else startY + 15\n",
    "                cv2.putText(frame, label, (startX, y),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # 処理結果のフレームを表示\n",
    "    cv2.imshow(\"Person Detection\", frame)\n",
    "\n",
    "    # キー入力を1ミリ秒待機し、'q'キーが押されたかチェック\n",
    "    # 0xFF == ord('q')は、押されたキーが'q'かどうかを確認\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break  # 'q'キーが押されたらループを終了\n",
    "        \n",
    "\n",
    "# リソースを解放\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【上級】MobileNetSSDの検出結果を用いた人追跡\n",
    "\n",
    "このPythonスクリプトは、リアルタイムの人物追跡システムを実装しています。主な機能は以下の通りです：\n",
    "\n",
    "1. OpenCVとMobileNet SSDモデルを使用して、カメラフィードから人物を検出します。\n",
    "2. 検出された人物に対して個別のトラッカーを割り当て、フレーム間で追跡を行います。\n",
    "3. 一定間隔で再検出を行い、新しい人物の出現や既存の追跡対象の位置を更新します。\n",
    "4. 各追跡対象に一意のIDと色を割り当て、バウンディングボックスと移動軌跡を表示します。\n",
    "5. データアソシエーションを使用して、検出結果と既存のトラッカーをマッチングします。\n",
    "データアソシエーションとは、複数のフレームや時間にわたって検出された物体を、同一の物体として関連付ける処理のことです。\n",
    "6. 追跡に失敗したオブジェクトを適切に管理し、システムの安定性を保ちます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2  \n",
    "import numpy as np  \n",
    "\n",
    "# 事前学習済みのMobileNet SSDモデルを人検出のためにロードします\n",
    "prototxt_path = 'det/MobileNetSSD_deploy.prototxt'  # モデルの構造を定義するプロトテキストファイル\n",
    "model_path = 'det/MobileNetSSD_deploy.caffemodel'   # 学習済みの重みが保存されたモデルファイル\n",
    "\n",
    "# MobileNet SSDが検出可能なクラスラベルのリストを初期化します\n",
    "# このリストには様々なオブジェクトが含まれていますが、このプログラムでは主に\"person\"を使用します\n",
    "CLASSES = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\",\n",
    "           \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\",\n",
    "           \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\",\n",
    "           \"sofa\", \"train\", \"tvmonitor\"]\n",
    "\n",
    "# OpenCVの深層学習モジュールを使用してモデルを読み込みます\n",
    "net = cv2.dnn.readNetFromCaffe(prototxt_path, model_path)\n",
    "\n",
    "# カメラからのビデオキャプチャを初期化します\n",
    "# 0はデフォルトのカメラを指し、cv2.CAP_DSHOWはDirectShowを使用することを指定します\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "\n",
    "# 追跡対象の管理に使用する変数を初期化します\n",
    "trackers = {}      # 各追跡対象のトラッカーを格納する辞書\n",
    "track_id = 0       # 追跡対象を一意に識別するためのID\n",
    "colors = {}        # 各追跡対象に割り当てる色を格納する辞書\n",
    "trajectories = {}  # 各追跡対象の移動軌跡を格納する辞書\n",
    "\n",
    "# フレームカウントと再検出の間隔を設定します\n",
    "frame_count = 0\n",
    "RE_DETECT_INTERVAL = 10  # 10フレームごとに再検出を行います\n",
    "\n",
    "# IoU（Intersection over Union）を計算する関数を定義します\n",
    "# IoUは2つのバウンディングボックスがどれだけ重なっているかを示す指標です\n",
    "def compute_iou(box1, box2):\n",
    "    \"\"\"2つのバウンディングボックスのIoUを計算します。\"\"\"\n",
    "    x1, y1, w1, h1 = box1\n",
    "    x1_max = x1 + w1\n",
    "    y1_max = y1 + h1\n",
    "\n",
    "    x2, y2, w2, h2 = box2\n",
    "    x2_max = x2 + w2\n",
    "    y2_max = y2 + h2\n",
    "\n",
    "    # 重なり部分の座標を計算します\n",
    "    xi1 = max(x1, x2)\n",
    "    yi1 = max(y1, y2)\n",
    "    xi2 = min(x1_max, x2_max)\n",
    "    yi2 = min(y1_max, y2_max)\n",
    "    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n",
    "\n",
    "    # 各ボックスの面積を計算します\n",
    "    box1_area = w1 * h1\n",
    "    box2_area = w2 * h2\n",
    "\n",
    "    # IoUを計算します\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "    if union_area == 0:\n",
    "        return 0.0  # ゼロ除算を避けるため\n",
    "\n",
    "    iou = inter_area / union_area\n",
    "    return iou\n",
    "\n",
    "# メインのループ処理を開始します\n",
    "while True:\n",
    "    # 新しいフレームを読み込みます\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break  # フレームが取得できなければループを抜けます\n",
    "\n",
    "    frame_count += 1  # フレームカウンタを増加させます\n",
    "\n",
    "    # フレームのサイズを取得します\n",
    "    (h, w) = frame.shape[:2]\n",
    "\n",
    "    # 一定間隔（RE_DETECT_INTERVAL）で再検出を行います\n",
    "    if frame_count % RE_DETECT_INTERVAL == 0:\n",
    "        # フレームを検出用に前処理します\n",
    "        blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)),\n",
    "                                     0.007843, (300, 300), 127.5)\n",
    "\n",
    "        # モデルに入力を設定し、推論を行います\n",
    "        net.setInput(blob)\n",
    "        detections = net.forward()\n",
    "\n",
    "        # 検出結果を格納するリストを初期化します\n",
    "        detections_list = []\n",
    "\n",
    "        # 検出結果をループ処理します\n",
    "        for i in range(detections.shape[2]):\n",
    "            confidence = detections[0, 0, i, 2]\n",
    "\n",
    "            # 信頼度が0.5以上の検出結果のみを処理します\n",
    "            if confidence > 0.5:\n",
    "                idx = int(detections[0, 0, i, 1])\n",
    "\n",
    "                # クラスが「person」の場合のみ処理します\n",
    "                if CLASSES[idx] == \"person\":\n",
    "                    # バウンディングボックスの座標を計算します\n",
    "                    box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "                    (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "                    # バウンディングボックスを（x, y, w, h）の形式に変換します\n",
    "                    bbox = (startX, startY, endX - startX, endY - startY)\n",
    "                    detections_list.append(bbox)\n",
    "\n",
    "        # データアソシエーション：検出結果と既存のトラッカーをマッチングします\n",
    "        unmatched_detections = detections_list.copy()\n",
    "        unmatched_trackers = list(trackers.keys())\n",
    "\n",
    "        # 各検出結果に対して、既存のトラッカーとマッチングを試みます\n",
    "        for det in detections_list:\n",
    "            best_iou = 0\n",
    "            best_tracker_id = None\n",
    "            for trk_id in unmatched_trackers:\n",
    "                # トラッカーの最後のバウンディングボックスを取得します\n",
    "                tracker = trackers[trk_id]['tracker']\n",
    "                trk_bbox = trackers[trk_id]['bbox']\n",
    "\n",
    "                # IoUを計算します\n",
    "                iou = compute_iou(det, trk_bbox)\n",
    "\n",
    "                # 最もIoUが高いトラッカーを見つけます\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_tracker_id = trk_id\n",
    "\n",
    "            # IoUが閾値を超える場合、トラッカーを更新します\n",
    "            if best_iou > 0.3:\n",
    "                # トラッカーを新しい検出結果で初期化します\n",
    "                tracker = trackers[best_tracker_id]['tracker']\n",
    "                ok = tracker.init(frame, det)\n",
    "                if ok:\n",
    "                    trackers[best_tracker_id]['bbox'] = det\n",
    "                    # 軌跡に現在の位置を追加します\n",
    "                    trackers[best_tracker_id]['trajectory'].append(\n",
    "                        (int(det[0] + det[2] / 2), int(det[1] + det[3] / 2)))\n",
    "                    unmatched_detections.remove(det)\n",
    "                    unmatched_trackers.remove(best_tracker_id)\n",
    "                else:\n",
    "                    # トラッカーの再初期化が失敗した場合、新しいトラッカーを作成します\n",
    "                    try:\n",
    "                        tracker = cv2.TrackerCSRT_create()\n",
    "                    except AttributeError:\n",
    "                        tracker = cv2.legacy.TrackerCSRT_create()\n",
    "                    tracker.init(frame, det)\n",
    "                    trackers[best_tracker_id]['tracker'] = tracker\n",
    "                    trackers[best_tracker_id]['bbox'] = det\n",
    "                    trackers[best_tracker_id]['trajectory'] = [\n",
    "                        (int(det[0] + det[2] / 2), int(det[1] + det[3] / 2))]\n",
    "                    unmatched_detections.remove(det)\n",
    "                    unmatched_trackers.remove(best_tracker_id)\n",
    "\n",
    "        # マッチしなかった検出結果について、新しいトラッカーを作成します\n",
    "        for det in unmatched_detections:\n",
    "            # 新しいトラッカーを作成します\n",
    "            try:\n",
    "                tracker = cv2.TrackerCSRT_create()\n",
    "            except AttributeError:\n",
    "                tracker = cv2.legacy.TrackerCSRT_create()\n",
    "            ok = tracker.init(frame, det)\n",
    "            if ok:\n",
    "                trackers[track_id] = {\n",
    "                    'tracker': tracker,\n",
    "                    'bbox': det,\n",
    "                    'trajectory': [(int(det[0] + det[2] / 2), int(det[1] + det[3] / 2))]\n",
    "                }\n",
    "                # ランダムな色を生成してトラッカーに割り当てます\n",
    "                colors[track_id] = (int(np.random.randint(0, 255)),\n",
    "                                    int(np.random.randint(0, 255)),\n",
    "                                    int(np.random.randint(0, 255)))\n",
    "                track_id += 1  # トラックIDを増加させます\n",
    "\n",
    "        # マッチしなかったトラッカーを削除します\n",
    "        for trk_id in unmatched_trackers:\n",
    "            del trackers[trk_id]\n",
    "            del colors[trk_id]\n",
    "\n",
    "    else:\n",
    "        # 既存のトラッカーを更新します\n",
    "        to_delete = []\n",
    "        for trk_id in list(trackers.keys()):\n",
    "            tracker = trackers[trk_id]['tracker']\n",
    "            ok, bbox = tracker.update(frame)\n",
    "            if ok:\n",
    "                # トラッキングが成功した場合、バウンディングボックスを更新します\n",
    "                trackers[trk_id]['bbox'] = bbox\n",
    "                (x, y, w, h) = [int(v) for v in bbox]\n",
    "                # バウンディングボックスを描画します\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), colors[trk_id], 2)\n",
    "\n",
    "                # トラックIDを表示します\n",
    "                cv2.putText(frame, f'ID {trk_id}', (x, y - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, colors[trk_id], 2)\n",
    "\n",
    "                # 軌跡を更新します\n",
    "                center = (int(x + w / 2), int(y + h / 2))\n",
    "                trackers[trk_id]['trajectory'].append(center)\n",
    "\n",
    "                # 軌跡を描画します\n",
    "                trajectory = trackers[trk_id]['trajectory']\n",
    "                for i in range(1, len(trajectory)):\n",
    "                    cv2.line(frame, trajectory[i - 1], trajectory[i], colors[trk_id], 2)\n",
    "            else:\n",
    "                # トラッキングが失敗した場合、削除リストに追加します\n",
    "                to_delete.append(trk_id)\n",
    "\n",
    "        # 失敗したトラッカーを削除します\n",
    "        for trk_id in to_delete:\n",
    "            del trackers[trk_id]\n",
    "            del colors[trk_id]\n",
    "\n",
    "    # フレームを表示します\n",
    "    cv2.imshow(\"Tracking\", frame)\n",
    "\n",
    "    # Qキーが押されたらループを終了します\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# リソースを解放します\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【上級】OpenPoseを用いた骨格検出\n",
    "\n",
    "このPythonスクリプトは、OpenCVとOpenPoseを使用して、リアルタイムの人体ポーズ推定を行います。主な機能は以下の通りです：\n",
    "\n",
    "1. ウェブカメラからのビデオ入力を取得します。\n",
    "2. 各フレームに対して、事前学習済みのOpenPoseモデル（COCOまたはMPI）を使用してポーズ推定を行います。\n",
    "3. 検出された人体の各キーポイント（関節など）を画像上に表示します。\n",
    "4. キーポイント間を線で結び、人体のスケルトンを描画します。\n",
    "5. 結果をリアルタイムで表示し、キー入力があるまで処理を継続します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリをインポート\n",
    "import cv2  # OpenCVライブラリ：画像処理用\n",
    "import time  # 時間計測用\n",
    "import numpy as np  # 数値計算用\n",
    "\n",
    "# ポーズ推定のモデルを選択（COCO or MPI）\n",
    "MODE = \"MPI\"\n",
    "\n",
    "# COCOモデルの設定\n",
    "if MODE == \"COCO\":\n",
    "    protoFile = \"pose/coco/pose_deploy_linevec.prototxt\"  # モデル構造ファイル\n",
    "    weightsFile = \"pose/coco/pose_iter_440000.caffemodel\"  # 学習済みの重みファイル\n",
    "    nPoints = 18  # キーポイントの数\n",
    "    # キーポイント間の接続を定義（スケルトンの描画に使用）\n",
    "    POSE_PAIRS = [ [1,0], [1,2], [1,5], [2,3], [3,4], [5,6],\n",
    "                   [6,7], [1,8], [8,9], [9,10], [1,11], [11,12],\n",
    "                   [12,13], [0,14], [0,15], [14,16], [15,17] ]\n",
    "\n",
    "# MPIモデルの設定\n",
    "elif MODE == \"MPI\":\n",
    "    protoFile = \"pose/mpi/pose_deploy_linevec_faster_4_stages.prototxt\"\n",
    "    weightsFile = \"pose/mpi/pose_iter_160000.caffemodel\"\n",
    "    nPoints = 15\n",
    "    POSE_PAIRS = [ [0,1], [1,2], [2,3], [3,4], [1,5],\n",
    "                   [5,6], [6,7], [1,14], [14,8], [8,9],\n",
    "                   [9,10], [14,11], [11,12], [12,13] ]\n",
    "\n",
    "# OpenPoseネットワークの読み込み\n",
    "net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)\n",
    "\n",
    "# ウェブカメラからの映像取得を初期化\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "\n",
    "# 入力画像のサイズを設定\n",
    "inWidth = 368\n",
    "inHeight = 368\n",
    "\n",
    "# キーポイント検出の閾値\n",
    "threshold = 0.1\n",
    "\n",
    "# メインループ：キー入力があるまで続く\n",
    "while cv2.waitKey(1) < 0:\n",
    "    t = time.time()  # フレーム処理時間の計測開始\n",
    "\n",
    "    # カメラからフレームを読み込む\n",
    "    hasFrame, frame = cap.read()\n",
    "    if not hasFrame:\n",
    "        break\n",
    "\n",
    "    # フレームのサイズを取得\n",
    "    frameWidth = frame.shape[1]\n",
    "    frameHeight = frame.shape[0]\n",
    "\n",
    "    # フレームのコピーを作成（キーポイント描画用）\n",
    "    frameCopy = np.copy(frame)\n",
    "\n",
    "    # OpenPoseの入力形式に合わせて画像を前処理\n",
    "    inpBlob = cv2.dnn.blobFromImage(frame, 1.0 / 255, (inWidth, inHeight),\n",
    "                                    (0, 0, 0), swapRB=False, crop=False)\n",
    "\n",
    "    # ネットワークに入力を設定\n",
    "    net.setInput(inpBlob)\n",
    "\n",
    "    # 推論を実行\n",
    "    output = net.forward()\n",
    "\n",
    "    # 出力のサイズを取得\n",
    "    H = output.shape[2]\n",
    "    W = output.shape[3]\n",
    "\n",
    "    # キーポイントの検出\n",
    "    points = []\n",
    "    for i in range(nPoints):\n",
    "        # 各キーポイントの確率マップを取得q\n",
    "        probMap = output[0, i, :, :]\n",
    "        \n",
    "        # 確率マップから最大値とその位置を取得\n",
    "        minVal, prob, minLoc, point = cv2.minMaxLoc(probMap)\n",
    "        \n",
    "        # 画像上の座標に変換\n",
    "        x = (frameWidth * point[0]) / W\n",
    "        y = (frameHeight * point[1]) / H\n",
    "\n",
    "        # 閾値以上の確率を持つキーポイントを描画\n",
    "        if prob > threshold:\n",
    "            cv2.circle(frameCopy, (int(x), int(y)), 8, (0, 255, 255),\n",
    "                       thickness=-1, lineType=cv2.FILLED)\n",
    "            cv2.putText(frameCopy, \"{}\".format(i), (int(x), int(y)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2,\n",
    "                        lineType=cv2.LINE_AA)\n",
    "            points.append((int(x), int(y)))\n",
    "        else:\n",
    "            points.append(None)\n",
    "\n",
    "    # スケルトンの描画\n",
    "    for pair in POSE_PAIRS:\n",
    "        partA = pair[0]\n",
    "        partB = pair[1]\n",
    "        if points[partA] and points[partB]:\n",
    "            cv2.line(frame, points[partA], points[partB],\n",
    "                     (0, 255, 255), 3)\n",
    "\n",
    "    # 結果の表示\n",
    "    cv2.imshow('Output-Keypoints', frameCopy)\n",
    "    cv2.imshow('Output-Skeleton', frame)\n",
    "\n",
    "# リソースの解放\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
