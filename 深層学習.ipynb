{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenCVとDlibを用いた画像認識 ｰ深層学習ｰ\n",
    "\n",
    "ここでは、カメラから取得した映像を用いて画像認識を行い、\n",
    "必要な情報を取得するための流れを学ぶことで、\n",
    "画像認識をビジネス現場で応用するイメージをつかみます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【上級】MobileNetSSDを用いた人の検出\n",
    "\n",
    "OpenCVとMobileNet SSDモデルを使用して、ビデオ内の人物を検出するプログラムです。主な機能は以下の通りです：\n",
    "\n",
    "1. 事前学習済みのMobileNet SSDモデルを読み込みます。\n",
    "2. 指定されたビデオファイルを開き、フレームごとに処理を行います。\n",
    "3. 各フレームに対して、MobileNet SSDモデルを使用して物体検出を実行します。\n",
    "4. 検出された物体の中から「人」のみを抽出し、バウンディングボックスと信頼度を表示します。\n",
    "5. 処理結果をリアルタイムで画面に表示します。\n",
    "6. Qキーが押されるまで処理を続け、終了時にはリソースを解放します。\n",
    "\n",
    "MobileNetSSDのは、次のページをしてください。\n",
    "\n",
    "[MobileNetSSD](Dhttps://github.com/chuanqi305/MobileNet-SSD/tree/master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# MobileNet SSDモデルの設定ファイルのパスを指定\n",
    "prototxt_path = 'det/MobileNetSSD_deploy.prototxt'\n",
    "model_path = 'det/MobileNetSSD_deploy.caffemodel'\n",
    "\n",
    "# MobileNet SSDが検出できるオブジェクトクラスのリスト\n",
    "CLASSES = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\",\n",
    "           \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\",\n",
    "           \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\",\n",
    "           \"sofa\", \"train\", \"tvmonitor\"]\n",
    "\n",
    "# 事前学習済みのMobileNet SSDモデルを読み込む\n",
    "net = cv2.dnn.readNetFromCaffe(prototxt_path, model_path)\n",
    "\n",
    "# ビデオファイルを開く\n",
    "cap = cv2.VideoCapture('vtest.avi')\n",
    "\n",
    "# メインループ：フレームごとに処理を行う\n",
    "while True:\n",
    "    # ビデオからフレームを1枚読み込む\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break  # ビデオの終わりに達したらループを抜ける\n",
    "\n",
    "    # フレームの高さと幅を取得\n",
    "    (h, w) = frame.shape[:2]\n",
    "\n",
    "    # 入力画像を前処理し、ニューラルネットワークに入力できる形式に変換\n",
    "    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)),\n",
    "                                 0.007843, (300, 300), 127.5)\n",
    "\n",
    "    # ニューラルネットワークに入力データをセットし、順伝播を実行\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "\n",
    "    # 検出結果をループで処理\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        # 信頼度が0.2より高い検出結果のみを処理\n",
    "        if confidence > 0.2:\n",
    "            idx = int(detections[0, 0, i, 1])\n",
    "            # 検出されたオブジェクトが「人」の場合のみ処理\n",
    "            if CLASSES[idx] == \"person\":\n",
    "                # バウンディングボックスの座標を計算\n",
    "                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "                (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "                # フレーム上にバウンディングボックスとラベルを描画\n",
    "                label = f\"Person: {confidence:.2f}\"\n",
    "                cv2.rectangle(frame, (startX, startY), (endX, endY),\n",
    "                              (0, 255, 0), 2)\n",
    "                y = startY - 15 if startY - 15 > 15 else startY + 15\n",
    "                cv2.putText(frame, label, (startX, y),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # 処理結果のフレームを表示\n",
    "    cv2.imshow(\"Person Detection\", frame)\n",
    "\n",
    "    # キー入力を1ミリ秒待機し、'q'キーが押されたかチェック\n",
    "    # 0xFF == ord('q')は、押されたキーが'q'かどうかを確認\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break  # 'q'キーが押されたらループを終了\n",
    "        \n",
    "\n",
    "# リソースを解放\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【上級】MobileNetSSDの検出結果を用いた人追跡\n",
    "\n",
    "このPythonスクリプトは、リアルタイムの人物追跡システムを実装しています。主な機能は以下の通りです：\n",
    "\n",
    "1. OpenCVとMobileNet SSDモデルを使用して、カメラフィードから人物を検出します。\n",
    "2. 検出された人物に対して個別のトラッカーを割り当て、フレーム間で追跡を行います。\n",
    "3. 一定間隔で再検出を行い、新しい人物の出現や既存の追跡対象の位置を更新します。\n",
    "4. 各追跡対象に一意のIDと色を割り当て、バウンディングボックスと移動軌跡を表示します。\n",
    "5. データアソシエーションを使用して、検出結果と既存のトラッカーをマッチングします。\n",
    "データアソシエーションとは、複数のフレームや時間にわたって検出された物体を、同一の物体として関連付ける処理のことです。\n",
    "6. 追跡に失敗したオブジェクトを適切に管理し、システムの安定性を保ちます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2  \n",
    "import numpy as np  \n",
    "\n",
    "# 事前学習済みのMobileNet SSDモデルを人検出のためにロードします\n",
    "prototxt_path = 'det/MobileNetSSD_deploy.prototxt'  # モデルの構造を定義するプロトテキストファイル\n",
    "model_path = 'det/MobileNetSSD_deploy.caffemodel'   # 学習済みの重みが保存されたモデルファイル\n",
    "\n",
    "# MobileNet SSDが検出可能なクラスラベルのリストを初期化します\n",
    "# このリストには様々なオブジェクトが含まれていますが、このプログラムでは主に\"person\"を使用します\n",
    "CLASSES = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\",\n",
    "           \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\",\n",
    "           \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\",\n",
    "           \"sofa\", \"train\", \"tvmonitor\"]\n",
    "\n",
    "# OpenCVの深層学習モジュールを使用してモデルを読み込みます\n",
    "net = cv2.dnn.readNetFromCaffe(prototxt_path, model_path)\n",
    "\n",
    "# カメラからのビデオキャプチャを初期化します\n",
    "# 0はデフォルトのカメラを指し、cv2.CAP_DSHOWはDirectShowを使用することを指定します\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "\n",
    "# 追跡対象の管理に使用する変数を初期化します\n",
    "trackers = {}      # 各追跡対象のトラッカーを格納する辞書\n",
    "track_id = 0       # 追跡対象を一意に識別するためのID\n",
    "colors = {}        # 各追跡対象に割り当てる色を格納する辞書\n",
    "trajectories = {}  # 各追跡対象の移動軌跡を格納する辞書\n",
    "\n",
    "# フレームカウントと再検出の間隔を設定します\n",
    "frame_count = 0\n",
    "RE_DETECT_INTERVAL = 10  # 10フレームごとに再検出を行います\n",
    "\n",
    "# IoU（Intersection over Union）を計算する関数を定義します\n",
    "# IoUは2つのバウンディングボックスがどれだけ重なっているかを示す指標です\n",
    "def compute_iou(box1, box2):\n",
    "    \"\"\"2つのバウンディングボックスのIoUを計算します。\"\"\"\n",
    "    x1, y1, w1, h1 = box1\n",
    "    x1_max = x1 + w1\n",
    "    y1_max = y1 + h1\n",
    "\n",
    "    x2, y2, w2, h2 = box2\n",
    "    x2_max = x2 + w2\n",
    "    y2_max = y2 + h2\n",
    "\n",
    "    # 重なり部分の座標を計算します\n",
    "    xi1 = max(x1, x2)\n",
    "    yi1 = max(y1, y2)\n",
    "    xi2 = min(x1_max, x2_max)\n",
    "    yi2 = min(y1_max, y2_max)\n",
    "    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n",
    "\n",
    "    # 各ボックスの面積を計算します\n",
    "    box1_area = w1 * h1\n",
    "    box2_area = w2 * h2\n",
    "\n",
    "    # IoUを計算します\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "    if union_area == 0:\n",
    "        return 0.0  # ゼロ除算を避けるため\n",
    "\n",
    "    iou = inter_area / union_area\n",
    "    return iou\n",
    "\n",
    "# メインのループ処理を開始します\n",
    "while True:\n",
    "    # 新しいフレームを読み込みます\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break  # フレームが取得できなければループを抜けます\n",
    "\n",
    "    frame_count += 1  # フレームカウンタを増加させます\n",
    "\n",
    "    # フレームのサイズを取得します\n",
    "    (h, w) = frame.shape[:2]\n",
    "\n",
    "    # 一定間隔（RE_DETECT_INTERVAL）で再検出を行います\n",
    "    if frame_count % RE_DETECT_INTERVAL == 0:\n",
    "        # フレームを検出用に前処理します\n",
    "        blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)),\n",
    "                                     0.007843, (300, 300), 127.5)\n",
    "\n",
    "        # モデルに入力を設定し、推論を行います\n",
    "        net.setInput(blob)\n",
    "        detections = net.forward()\n",
    "\n",
    "        # 検出結果を格納するリストを初期化します\n",
    "        detections_list = []\n",
    "\n",
    "        # 検出結果をループ処理します\n",
    "        for i in range(detections.shape[2]):\n",
    "            confidence = detections[0, 0, i, 2]\n",
    "\n",
    "            # 信頼度が0.5以上の検出結果のみを処理します\n",
    "            if confidence > 0.5:\n",
    "                idx = int(detections[0, 0, i, 1])\n",
    "\n",
    "                # クラスが「person」の場合のみ処理します\n",
    "                if CLASSES[idx] == \"person\":\n",
    "                    # バウンディングボックスの座標を計算します\n",
    "                    box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "                    (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "                    # バウンディングボックスを（x, y, w, h）の形式に変換します\n",
    "                    bbox = (startX, startY, endX - startX, endY - startY)\n",
    "                    detections_list.append(bbox)\n",
    "\n",
    "        # データアソシエーション：検出結果と既存のトラッカーをマッチングします\n",
    "        unmatched_detections = detections_list.copy()\n",
    "        unmatched_trackers = list(trackers.keys())\n",
    "\n",
    "        # 各検出結果に対して、既存のトラッカーとマッチングを試みます\n",
    "        for det in detections_list:\n",
    "            best_iou = 0\n",
    "            best_tracker_id = None\n",
    "            for trk_id in unmatched_trackers:\n",
    "                # トラッカーの最後のバウンディングボックスを取得します\n",
    "                tracker = trackers[trk_id]['tracker']\n",
    "                trk_bbox = trackers[trk_id]['bbox']\n",
    "\n",
    "                # IoUを計算します\n",
    "                iou = compute_iou(det, trk_bbox)\n",
    "\n",
    "                # 最もIoUが高いトラッカーを見つけます\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_tracker_id = trk_id\n",
    "\n",
    "            # IoUが閾値を超える場合、トラッカーを更新します\n",
    "            if best_iou > 0.3:\n",
    "                # トラッカーを新しい検出結果で初期化します\n",
    "                tracker = trackers[best_tracker_id]['tracker']\n",
    "                ok = tracker.init(frame, det)\n",
    "                if ok:\n",
    "                    trackers[best_tracker_id]['bbox'] = det\n",
    "                    # 軌跡に現在の位置を追加します\n",
    "                    trackers[best_tracker_id]['trajectory'].append(\n",
    "                        (int(det[0] + det[2] / 2), int(det[1] + det[3] / 2)))\n",
    "                    unmatched_detections.remove(det)\n",
    "                    unmatched_trackers.remove(best_tracker_id)\n",
    "                else:\n",
    "                    # トラッカーの再初期化が失敗した場合、新しいトラッカーを作成します\n",
    "                    try:\n",
    "                        tracker = cv2.TrackerCSRT_create()\n",
    "                    except AttributeError:\n",
    "                        tracker = cv2.legacy.TrackerCSRT_create()\n",
    "                    tracker.init(frame, det)\n",
    "                    trackers[best_tracker_id]['tracker'] = tracker\n",
    "                    trackers[best_tracker_id]['bbox'] = det\n",
    "                    trackers[best_tracker_id]['trajectory'] = [\n",
    "                        (int(det[0] + det[2] / 2), int(det[1] + det[3] / 2))]\n",
    "                    unmatched_detections.remove(det)\n",
    "                    unmatched_trackers.remove(best_tracker_id)\n",
    "\n",
    "        # マッチしなかった検出結果について、新しいトラッカーを作成します\n",
    "        for det in unmatched_detections:\n",
    "            # 新しいトラッカーを作成します\n",
    "            try:\n",
    "                tracker = cv2.TrackerCSRT_create()\n",
    "            except AttributeError:\n",
    "                tracker = cv2.legacy.TrackerCSRT_create()\n",
    "            ok = tracker.init(frame, det)\n",
    "            if ok:\n",
    "                trackers[track_id] = {\n",
    "                    'tracker': tracker,\n",
    "                    'bbox': det,\n",
    "                    'trajectory': [(int(det[0] + det[2] / 2), int(det[1] + det[3] / 2))]\n",
    "                }\n",
    "                # ランダムな色を生成してトラッカーに割り当てます\n",
    "                colors[track_id] = (int(np.random.randint(0, 255)),\n",
    "                                    int(np.random.randint(0, 255)),\n",
    "                                    int(np.random.randint(0, 255)))\n",
    "                track_id += 1  # トラックIDを増加させます\n",
    "\n",
    "        # マッチしなかったトラッカーを削除します\n",
    "        for trk_id in unmatched_trackers:\n",
    "            del trackers[trk_id]\n",
    "            del colors[trk_id]\n",
    "\n",
    "    else:\n",
    "        # 既存のトラッカーを更新します\n",
    "        to_delete = []\n",
    "        for trk_id in list(trackers.keys()):\n",
    "            tracker = trackers[trk_id]['tracker']\n",
    "            ok, bbox = tracker.update(frame)\n",
    "            if ok:\n",
    "                # トラッキングが成功した場合、バウンディングボックスを更新します\n",
    "                trackers[trk_id]['bbox'] = bbox\n",
    "                (x, y, w, h) = [int(v) for v in bbox]\n",
    "                # バウンディングボックスを描画します\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), colors[trk_id], 2)\n",
    "\n",
    "                # トラックIDを表示します\n",
    "                cv2.putText(frame, f'ID {trk_id}', (x, y - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, colors[trk_id], 2)\n",
    "\n",
    "                # 軌跡を更新します\n",
    "                center = (int(x + w / 2), int(y + h / 2))\n",
    "                trackers[trk_id]['trajectory'].append(center)\n",
    "\n",
    "                # 軌跡を描画します\n",
    "                trajectory = trackers[trk_id]['trajectory']\n",
    "                for i in range(1, len(trajectory)):\n",
    "                    cv2.line(frame, trajectory[i - 1], trajectory[i], colors[trk_id], 2)\n",
    "            else:\n",
    "                # トラッキングが失敗した場合、削除リストに追加します\n",
    "                to_delete.append(trk_id)\n",
    "\n",
    "        # 失敗したトラッカーを削除します\n",
    "        for trk_id in to_delete:\n",
    "            del trackers[trk_id]\n",
    "            del colors[trk_id]\n",
    "\n",
    "    # フレームを表示します\n",
    "    cv2.imshow(\"Tracking\", frame)\n",
    "\n",
    "    # Qキーが押されたらループを終了します\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# リソースを解放します\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【上級】OpenPoseを用いた骨格検出\n",
    "\n",
    "このPythonスクリプトは、OpenCVとOpenPoseを使用して、リアルタイムの人体ポーズ推定を行います。主な機能は以下の通りです：\n",
    "\n",
    "1. ウェブカメラからのビデオ入力を取得します。\n",
    "2. 各フレームに対して、事前学習済みのOpenPoseモデル（COCOまたはMPI）を使用してポーズ推定を行います。\n",
    "3. 検出された人体の各キーポイント（関節など）を画像上に表示します。\n",
    "4. キーポイント間を線で結び、人体のスケルトンを描画します。\n",
    "5. 結果をリアルタイムで表示し、キー入力があるまで処理を継続します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリをインポート\n",
    "import cv2  # OpenCVライブラリ：画像処理用\n",
    "import time  # 時間計測用\n",
    "import numpy as np  # 数値計算用\n",
    "\n",
    "# ポーズ推定のモデルを選択（COCO or MPI）\n",
    "MODE = \"MPI\"\n",
    "\n",
    "# COCOモデルの設定\n",
    "if MODE == \"COCO\":\n",
    "    protoFile = \"pose/coco/pose_deploy_linevec.prototxt\"  # モデル構造ファイル\n",
    "    weightsFile = \"pose/coco/pose_iter_440000.caffemodel\"  # 学習済みの重みファイル\n",
    "    nPoints = 18  # キーポイントの数\n",
    "    # キーポイント間の接続を定義（スケルトンの描画に使用）\n",
    "    POSE_PAIRS = [ [1,0], [1,2], [1,5], [2,3], [3,4], [5,6],\n",
    "                   [6,7], [1,8], [8,9], [9,10], [1,11], [11,12],\n",
    "                   [12,13], [0,14], [0,15], [14,16], [15,17] ]\n",
    "\n",
    "# MPIモデルの設定\n",
    "elif MODE == \"MPI\":\n",
    "    protoFile = \"pose/mpi/pose_deploy_linevec_faster_4_stages.prototxt\"\n",
    "    weightsFile = \"pose/mpi/pose_iter_160000.caffemodel\"\n",
    "    nPoints = 15\n",
    "    POSE_PAIRS = [ [0,1], [1,2], [2,3], [3,4], [1,5],\n",
    "                   [5,6], [6,7], [1,14], [14,8], [8,9],\n",
    "                   [9,10], [14,11], [11,12], [12,13] ]\n",
    "\n",
    "# OpenPoseネットワークの読み込み\n",
    "net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)\n",
    "\n",
    "# ウェブカメラからの映像取得を初期化\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "\n",
    "# 入力画像のサイズを設定\n",
    "inWidth = 368\n",
    "inHeight = 368\n",
    "\n",
    "# キーポイント検出の閾値\n",
    "threshold = 0.1\n",
    "\n",
    "# メインループ：キー入力があるまで続く\n",
    "while cv2.waitKey(1) < 0:\n",
    "    t = time.time()  # フレーム処理時間の計測開始\n",
    "\n",
    "    # カメラからフレームを読み込む\n",
    "    hasFrame, frame = cap.read()\n",
    "    if not hasFrame:\n",
    "        break\n",
    "\n",
    "    # フレームのサイズを取得\n",
    "    frameWidth = frame.shape[1]\n",
    "    frameHeight = frame.shape[0]\n",
    "\n",
    "    # フレームのコピーを作成（キーポイント描画用）\n",
    "    frameCopy = np.copy(frame)\n",
    "\n",
    "    # OpenPoseの入力形式に合わせて画像を前処理\n",
    "    inpBlob = cv2.dnn.blobFromImage(frame, 1.0 / 255, (inWidth, inHeight),\n",
    "                                    (0, 0, 0), swapRB=False, crop=False)\n",
    "\n",
    "    # ネットワークに入力を設定\n",
    "    net.setInput(inpBlob)\n",
    "\n",
    "    # 推論を実行\n",
    "    output = net.forward()\n",
    "\n",
    "    # 出力のサイズを取得\n",
    "    H = output.shape[2]\n",
    "    W = output.shape[3]\n",
    "\n",
    "    # キーポイントの検出\n",
    "    points = []\n",
    "    for i in range(nPoints):\n",
    "        # 各キーポイントの確率マップを取得q\n",
    "        probMap = output[0, i, :, :]\n",
    "        \n",
    "        # 確率マップから最大値とその位置を取得\n",
    "        minVal, prob, minLoc, point = cv2.minMaxLoc(probMap)\n",
    "        \n",
    "        # 画像上の座標に変換\n",
    "        x = (frameWidth * point[0]) / W\n",
    "        y = (frameHeight * point[1]) / H\n",
    "\n",
    "        # 閾値以上の確率を持つキーポイントを描画\n",
    "        if prob > threshold:\n",
    "            cv2.circle(frameCopy, (int(x), int(y)), 8, (0, 255, 255),\n",
    "                       thickness=-1, lineType=cv2.FILLED)\n",
    "            cv2.putText(frameCopy, \"{}\".format(i), (int(x), int(y)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2,\n",
    "                        lineType=cv2.LINE_AA)\n",
    "            points.append((int(x), int(y)))\n",
    "        else:\n",
    "            points.append(None)\n",
    "\n",
    "    # スケルトンの描画\n",
    "    for pair in POSE_PAIRS:\n",
    "        partA = pair[0]\n",
    "        partB = pair[1]\n",
    "        if points[partA] and points[partB]:\n",
    "            cv2.line(frame, points[partA], points[partB],\n",
    "                     (0, 255, 255), 3)\n",
    "\n",
    "    # 結果の表示\n",
    "    cv2.imshow('Output-Keypoints', frameCopy)\n",
    "    cv2.imshow('Output-Skeleton', frame)\n",
    "\n",
    "# リソースの解放\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Darknet (Yolov3)\n",
    "\n",
    "YOLOv3は、高精度かつ高速な物体検出が可能なディープニューラルネットワーク（DNN）で、多くの応用で知られています。トレーニング方法やモデルの詳細は[こちらのページ](https://github.com/AlexeyAB/darknet)をご覧ください。\n",
    "\n",
    "* `yolov3.cfg`を[こちらから](https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg)ダウンロードしてください\n",
    "* `yolov3.weight`を[こちらから](https://pjreddie.com/media/files/yolov3-spp.weights)ダウンロードしてください\n",
    "* `coco.names`を[こちらから](https://github.com/pjreddie/darknet/blob/master/data/coco.names)ダウンロードしてください\n",
    "\n",
    "`yolov3.cfg`、`yolov3.weight`、および`coco.names`を`det`フォルダに保存してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# YOLOv3-tinyモデルの読み込み\n",
    "net = cv2.dnn.readNetFromDarknet('det/yolov3.cfg', 'det/yolov3.weights')\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "\n",
    "# クラス名 (coco.namesファイルから読み込む)\n",
    "with open('det/coco.names', 'r') as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# 出力層の名前を取得\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# カメラを起動 (0はデフォルトのカメラデバイス)\n",
    "# cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "cap = cv2.VideoCapture('vtest.avi')\n",
    "\n",
    "while True:\n",
    "    # カメラからフレームを読み込む\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        print(\"カメラからのフレームの取得に失敗しました。\")\n",
    "        break\n",
    "    \n",
    "    height, width = frame.shape[:2]\n",
    "    \n",
    "    # YOLO用に画像を前処理 (416x416サイズにリサイズ)\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1/255.0, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    \n",
    "    # YOLOによる検出\n",
    "    layer_outputs = net.forward(output_layers)\n",
    "    \n",
    "    # 検出されたバウンディングボックス、信頼度、クラスIDを保存するリスト\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    class_ids = []\n",
    "    \n",
    "    # 各出力レイヤーをループ処理\n",
    "    for output in layer_outputs:\n",
    "        for detection in output:\n",
    "            # クラスIDと信頼度(確率)を抽出\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            \n",
    "            # 信頼度が0.5以上のものをフィルタリング\n",
    "            if confidence > 0.5:\n",
    "                # YOLOはバウンディングボックスの中心座標、幅、高さを返す\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                \n",
    "                # バウンディングボックスの左上の座標を計算\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                \n",
    "                # バウンディングボックス、信頼度、クラスIDをリストに追加\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "    \n",
    "    # 非最大抑制を使って重複するボックスを削除\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, score_threshold=0.5, nms_threshold=0.4)\n",
    "    \n",
    "    # 検出されたオブジェクトをフレームに描画\n",
    "    if len(indices) > 0:\n",
    "        for i in indices.flatten():\n",
    "            x, y, w, h = boxes[i]\n",
    "            color = (0, 255, 0)  # 緑色のボックス\n",
    "            label = f\"{classes[class_ids[i]]}: {confidences[i]:.2f}\"\n",
    "\n",
    "            # バウンディングボックスを描画\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "\n",
    "            # ラベルを描画\n",
    "            cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "    \n",
    "    # フレームを表示\n",
    "    cv2.imshow(\"Real-Time Object Detection\", frame)\n",
    "    \n",
    "    # 'q'キーが押されたらループを終了\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# カメラとウィンドウを解放\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Darknet (Yolov5)\n",
    "\n",
    "YOLOv5は、Ultralytics社によって開発された、YOLOv3を改良したリアルタイム物体検出アルゴリズムです。高速かつ高精度な検出が可能です。軽量なモデルから高精度なモデルまで、サイズの異なる複数のモデルが提供されており、簡単にカスタマイズや学習ができるため、様々な用途に適しています。YOLOv5に関する情報は豊富で、自作の物体検出器を構築する際には、v5の利用をおすすめします。\n",
    "\n",
    "**本プログラムを実行する際には、OpenCVのバージョンアップデートが必要なため、GitHub上の講義資料に従って専用の環境構築を行ってください。**\n",
    "\n",
    "このプログラムはOpenCVの[参考資料](https://learnopencv.com/object-detection-using-yolov5-and-opencv-dnn-in-c-and-python/)を元に作成されています．\n",
    "\n",
    "このプログラムを実行するためには、学習済みのDNNモデルが必要です。Google Colab上で以下のコードを順に実行して、`yolov5s.onnx`を作成してダウンロードしてください。または、[ここから](https://kutc-my.sharepoint.com/:u:/g/personal/t160024_kansai-u_ac_jp/ETb0zCwQVPlPo5-dPP1HitIB1erpb53UGZzaZkHQgyQuhA?e=CAGSPG)ダウンロードしてください。保存先は、フォルダ`det/`です。\n",
    "\n",
    "```\n",
    "!git clone https://github.com/ultralytics/yolov5\n",
    "%cd /content/yolov5/\n",
    "!pip install -r requirements.txt\n",
    "!wget https://github.com/ultralytics/YOLOv5/releases/download/v6.1/YOLOv5s.pt\n",
    "!python export.py --weights YOLOv5s.pt --opset 12 --include onnx\n",
    "```\n",
    "\n",
    "その他の参考資料\n",
    "\n",
    "[https://github.com/ultralytics/yolov5?tab=readme-ov-file](https://github.com/ultralytics/yolov5?tab=readme-ov-file)\n",
    "\n",
    "[https://docs.ultralytics.com/ja](https://docs.ultralytics.com/ja)\n",
    "\n",
    "[https://docs.ultralytics.com/ja/yolov5/tutorials/model_export/](https://docs.ultralytics.com/ja/yolov5/tutorials/model_export/)\n",
    "\n",
    "[https://qiita.com/02130515/items/cfbb9dcd8291418476ab](https://qiita.com/02130515/items/cfbb9dcd8291418476ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# 定数設定\n",
    "INPUT_WIDTH = 640  # 入力画像の幅\n",
    "INPUT_HEIGHT = 640  # 入力画像の高さ\n",
    "SCORE_THRESHOLD = 0.5  # スコアのしきい値\n",
    "NMS_THRESHOLD = 0.45  # 非最大抑制のしきい値\n",
    "CONFIDENCE_THRESHOLD = 0.45  # 信頼度のしきい値\n",
    "\n",
    "# テキスト表示に関する設定\n",
    "FONT_FACE = cv2.FONT_HERSHEY_SIMPLEX\n",
    "FONT_SCALE = 0.7  # フォントのサイズ\n",
    "THICKNESS = 1  # テキストの太さ\n",
    "\n",
    "# 色の設定\n",
    "BLACK  = (0, 0, 0)\n",
    "BLUE   = (255, 178, 50)\n",
    "YELLOW = (0, 255, 255)\n",
    "RED = (0, 0, 255)\n",
    "\n",
    "# ラベルを描画する関数\n",
    "def draw_label(input_image, label, left, top):\n",
    "    \"\"\"ラベル（テキスト）を指定の位置に描画する\"\"\"\n",
    "    \n",
    "    # テキストのサイズを取得\n",
    "    text_size = cv2.getTextSize(label, FONT_FACE, FONT_SCALE, THICKNESS)\n",
    "    dim, baseline = text_size[0], text_size[1]\n",
    "    # テキストの背景用の黒い矩形を描画\n",
    "    cv2.rectangle(input_image, (left, top), (left + dim[0], top + dim[1] + baseline), BLACK, cv2.FILLED)\n",
    "    # 矩形の中にテキストを描画\n",
    "    cv2.putText(input_image, label, (left, top + dim[1]), FONT_FACE, FONT_SCALE, YELLOW, THICKNESS, cv2.LINE_AA)\n",
    "\n",
    "# 前処理を行う関数\n",
    "def pre_process(input_image, net):\n",
    "    \"\"\"画像をネットワークに入力するための前処理\"\"\"\n",
    "    \n",
    "    # 画像をBLOB形式に変換\n",
    "    blob = cv2.dnn.blobFromImage(input_image, 1/255, (INPUT_WIDTH, INPUT_HEIGHT), [0,0,0], 1, crop=False)\n",
    "\n",
    "    # ネットワークに入力をセット\n",
    "    net.setInput(blob)\n",
    "\n",
    "    # 出力レイヤーの名前を取得し、フォワードパスで結果を取得\n",
    "    output_layers = net.getUnconnectedOutLayersNames()\n",
    "    outputs = net.forward(output_layers)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "# 後処理を行う関数\n",
    "def post_process(input_image, outputs, classes):\n",
    "    \"\"\"ネットワークの出力結果に基づいて後処理を行い、物体を検出して描画する\"\"\"\n",
    "    \n",
    "    # クラスID、信頼度、バウンディングボックスを保存するリスト\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    # 出力結果の行数を取得\n",
    "    rows = outputs[0].shape[1]\n",
    "\n",
    "    # 入力画像のサイズを取得\n",
    "    image_height, image_width = input_image.shape[:2]\n",
    "\n",
    "    # リサイズ係数の計算\n",
    "    x_factor = image_width / INPUT_WIDTH\n",
    "    y_factor = image_height / INPUT_HEIGHT\n",
    "\n",
    "    # 各検出結果を処理\n",
    "    for r in range(rows):\n",
    "        row = outputs[0][0][r]\n",
    "        confidence = row[4]  # 信頼度を取得\n",
    "\n",
    "        # 信頼度が閾値を超えた場合のみ処理\n",
    "        if confidence >= CONFIDENCE_THRESHOLD:\n",
    "            classes_scores = row[5:]  # 各クラスのスコアを取得\n",
    "\n",
    "            # スコアが最も高いクラスIDを取得\n",
    "            class_id = np.argmax(classes_scores)\n",
    "\n",
    "            # スコアがしきい値を超える場合のみ処理\n",
    "            if (classes_scores[class_id] > SCORE_THRESHOLD):\n",
    "                confidences.append(confidence)\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "                # バウンディングボックスの中心座標、幅、高さを取得\n",
    "                cx, cy, w, h = row[0], row[1], row[2], row[3]\n",
    "\n",
    "                # 左上の座標を計算\n",
    "                left = int((cx - w/2) * x_factor)\n",
    "                top = int((cy - h/2) * y_factor)\n",
    "                width = int(w * x_factor)\n",
    "                height = int(h * y_factor)\n",
    "\n",
    "                # バウンディングボックスを保存\n",
    "                box = np.array([left, top, width, height])\n",
    "                boxes.append(box)\n",
    "\n",
    "    # 非最大抑制を使用して重複するボックスを除去\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, CONFIDENCE_THRESHOLD, NMS_THRESHOLD)\n",
    "    for i in indices:\n",
    "        box = boxes[i]\n",
    "        left = box[0]\n",
    "        top = box[1]\n",
    "        width = box[2]\n",
    "        height = box[3]\n",
    "        # バウンディングボックスを描画\n",
    "        cv2.rectangle(input_image, (left, top), (left + width, top + height), BLUE, 3*THICKNESS)\n",
    "        # ラベルを描画\n",
    "        label = \"{}:{:.2f}\".format(classes[class_ids[i]], confidences[i])\n",
    "        draw_label(input_image, label, left, top)\n",
    "\n",
    "    return input_image\n",
    "\n",
    "# カメラ映像を使ったリアルタイム物体検出\n",
    "def run_real_time_detection():\n",
    "    \"\"\"カメラを使用してリアルタイムで物体検出を行う\"\"\"\n",
    "\n",
    "    # クラス名を読み込み\n",
    "    classesFile = \"det/coco.names\"\n",
    "    classes = None\n",
    "    with open(classesFile, 'rt') as f:\n",
    "        classes = f.read().rstrip('\\n').split('\\n')\n",
    "\n",
    "    # YOLOv5モデルを読み込む\n",
    "    modelWeights = \"det/yolov5s.onnx\"\n",
    "    net = cv2.dnn.readNet(modelWeights)\n",
    "\n",
    "    # デフォルトカメラ（0）を使用して映像を取得\n",
    "    cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "\n",
    "    while True:\n",
    "        # カメラからフレームをキャプチャ\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"カメラからフレームを取得できませんでした。\")\n",
    "            break\n",
    "\n",
    "        # 前処理を行い、検出を実行\n",
    "        detections = pre_process(frame, net)\n",
    "\n",
    "        # 検出結果に基づいて後処理を行い、フレームに描画\n",
    "        processed_frame = post_process(frame.copy(), detections, classes)\n",
    "\n",
    "        # 検出結果を別ウィンドウで表示\n",
    "        cv2.imshow('YOLOv5 Object Detection', processed_frame)\n",
    "\n",
    "        # 'q'キーを押すと終了\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # 終了時にカメラを解放し、ウィンドウを閉じる\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# リアルタイム物体検出を実行\n",
    "run_real_time_detection()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
