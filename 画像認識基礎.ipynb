{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenCVとDlibを用いた画像認識\n",
    "\n",
    "ここでは、カメラから取得した映像を用いて画像認識を行い、\n",
    "必要な情報を取得するための流れを学ぶことで、\n",
    "画像認識をビジネス現場で応用するイメージをつかみます。\n",
    "\n",
    "以下のコードを参考に、画像認識の基本的な操作を理解してください。\n",
    "各教材を解きながら、OpenCVやDlibの使い方、画像処理の流れを学んでください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 画像データを読み込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = cv2.imread(\"img/img01.jpg\")\n",
    "height, width = img.shape[:2]\n",
    "print(\"画像幅: \" + str(width))\n",
    "print(\"画像高さ: \" + str(height))\n",
    "\n",
    "# OpenCV が画像を扱う際に色の並びが RGB ではなく BGR\n",
    "# 画像を matplotlib で表示させる前に BGR の並びを RGB に変換する\n",
    "converted_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(converted_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ファイルから映像データを読み込む\n",
    "\n",
    "以下の主要な部分があります：\n",
    "\n",
    "1. 動画ファイルを開く\n",
    "2. 動画の基本情報（幅、高さ、フレーム数、FPS）の取得と表示\n",
    "3. メインループでの動画処理：\n",
    "    * フレームの読み込み\n",
    "    * フレームのリサイズ\n",
    "    * リサイズしたフレームの表示\n",
    "4. 'q'キーでループ終了\n",
    "5. リソースの解放"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリをインポート\n",
    "import cv2 \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# 動画ファイルを開く\n",
    "cap = cv2.VideoCapture(\"vtest.avi\")  # 'vtest.avi' という名前の動画ファイルを開く\n",
    "\n",
    "# 動画の基本情報を取得\n",
    "width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)  # フレームの幅を取得\n",
    "height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)  # フレームの高さを取得\n",
    "count = cap.get(cv2.CAP_PROP_FRAME_COUNT)  # 総フレーム数を取得\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)  # フレームレート（FPS）を取得\n",
    "\n",
    "# 取得した動画情報を表示\n",
    "print(\"画像幅: \" + str(width)) \n",
    "print(\"画像高さ: \" + str(height)) \n",
    "print(\"総フレーム数: \" + str(count)) \n",
    "print(\"FPS: \" + str(fps)) \n",
    "\n",
    "# メインループ：動画の各フレームを処理\n",
    "while(cap.isOpened()): \n",
    "    ret, frame = cap.read()  # 1フレームずつ読み込む。retはフレームの読み込みが成功したかどうか、frameは読み込んだフレーム\n",
    "    \n",
    "    if ret:  # フレームの読み込みが成功した場合\n",
    "        # フレームのサイズを半分に縮小\n",
    "        resized_frame = cv2.resize(frame, (int(width // 2), int(height // 2)))\n",
    "        \n",
    "        # 縮小したフレームを表示\n",
    "        cv2.imshow(\"frame\", resized_frame) \n",
    "    \n",
    "    # 'q'キーが押されたらループを終了\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "        break \n",
    "\n",
    "# リソースの解放\n",
    "cap.release()  # VideoCaptureオブジェクトを解放\n",
    "cv2.destroyAllWindows()  # すべてのOpenCVウィンドウを閉じる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webカメラから映像をデータを読み込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCVライブラリをインポート\n",
    "import cv2\n",
    "\n",
    "# Webカメラ映像のキャプチャを開始\n",
    "# 0は通常、PCに内蔵されているカメラを指す。外付けカメラを使用する場合は1や2などに変更する\n",
    "# cv2.CAP_DSHOWは、Windowsでのパフォーマンス向上のために使用\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "\n",
    "# キャプチャのプロパティを取得\n",
    "width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)   # フレームの幅を取得\n",
    "height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT) # フレームの高さを取得\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)             # フレームレート（FPS）を取得\n",
    "\n",
    "# 取得したカメラのプロパティを表示\n",
    "print(\"画像幅: \" + str(width))\n",
    "print(\"画像高さ: \" + str(height))\n",
    "print(\"FPS: \" + str(fps))\n",
    "\n",
    "# Webカメラからの映像を連続的に取得し表示するループ\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()  # フレームを1つ取得。retは取得成功フラグ、frameは取得した画像\n",
    "    \n",
    "    if ret:  # フレームの取得に成功した場合        \n",
    "        # 取得したフレームを表示\n",
    "        # \"Webcam\"は表示ウィンドウの名前\n",
    "        cv2.imshow(\"Webcam\", frame)\n",
    "        \n",
    "    # キー入力を1ミリ秒待機し、'q'キーが押されたかチェック\n",
    "    # 0xFF == ord('q')は、押されたキーが'q'かどうかを確認\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break  # 'q'キーが押されたらループを終了\n",
    "\n",
    "# キャプチャに使用したリソースを解放\n",
    "cap.release()\n",
    "\n",
    "# 作成したすべてのOpenCVウィンドウを閉じる\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 映像を画像に分割し，保存する\n",
    "\n",
    "主な構成要素は以下の通りです：\n",
    "\n",
    "1. 動画ファイルを開く\n",
    "2. フレーム番号の初期化\n",
    "3. メインループでの動画処理：\n",
    "    * フレームの読み込み\n",
    "    * フレームの表示\n",
    "    * スナップショットの保存\n",
    "    * 'q'キーでループ終了のチェック\n",
    "    * フレーム番号の更新\n",
    "4. リソースの解放とウィンドウを閉じる\n",
    "\n",
    "注意点：\n",
    "1. このスクリプトは、カレントディレクトリに\"snapshot\"フォルダが存在することを前提としています。フォルダが存在しない場合、エラーが発生する可能性があります。\n",
    "2. 動画の各フレームが保存されるため、長い動画や高解像度の動画の場合、大量のディスク容量を使用する可能性があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCVライブラリをインポート\n",
    "import cv2\n",
    "\n",
    "# 動画ファイルを開く\n",
    "cap = cv2.VideoCapture(\"vtest.avi\")  # 'vtest.avi' という名前の動画ファイルを開く\n",
    "\n",
    "# フレーム番号の初期化\n",
    "num = 0\n",
    "\n",
    "# 動画ファイルが正常に開かれている間、ループを続ける\n",
    "while(cap.isOpened()):\n",
    "    # フレームを1つ読み込む\n",
    "    # retは読み込み成功フラグ、frameは読み込んだ画像データ\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if ret:  # フレームの読み込みに成功した場合\n",
    "        # 読み込んだフレームを表示\n",
    "        cv2.imshow(\"frame\", frame)\n",
    "        \n",
    "        # スナップショットの保存先ファイルパスを生成\n",
    "        # ファイル名は\"snapshot_数字.jpg\"の形式\n",
    "        filepath = f\"snapshot/snapshot_{num}.jpg\"\n",
    "        \n",
    "        # 現在のフレームをJPEG画像として保存\n",
    "        cv2.imwrite(filepath, frame)\n",
    "        \n",
    "        # 'q'キーが押されたらループを終了\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # フレーム番号を1増やす\n",
    "    num += 1\n",
    "\n",
    "# VideoCaptureオブジェクトを解放\n",
    "cap.release()\n",
    "\n",
    "# すべてのOpenCVウィンドウを閉じる\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 画像内のどこに人がいるのかを検出する\n",
    "\n",
    "OpenCVを使用して画像から人物を検出するためには，一般的にHOG (Histogram of Oriented Gradients) とSVM (Support Vector Machine) を使用した人物検出がよく利用されます．以下のコードは，OpenCVのHOGDescriptorを用いて画像から人物を検出する例です．\n",
    "\n",
    "主な構成要素は以下の通りです：\n",
    "1. OpenCVライブラリのインポート\n",
    "2. HOG検出器の準備と設定\n",
    "3. 画像の読み込みとグレースケール変換\n",
    "4. 人物検出の実行\n",
    "5. 検出結果の処理（矩形描画）\n",
    "6. 結果の保存\n",
    "\n",
    "注意点：\n",
    "1. 検出結果は\"temp.jpg\"という名前で保存されます。既存のファイルがある場合は上書きされます。\n",
    "2. HOGパラメータは画像や検出対象によって調整が必要な場合があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCVライブラリをインポート\n",
    "import cv2\n",
    "\n",
    "# HOG (Histogram of Oriented Gradients) 検出器の準備\n",
    "# HOGは人物検出によく使われる特徴量です\n",
    "hog = cv2.HOGDescriptor()\n",
    "\n",
    "# デフォルトの人物検出用SVMを設定\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "# HOG検出器のパラメータを設定\n",
    "hogParams = {\n",
    "    'winStride': (8, 8),    # 検出ウィンドウの移動ステップ\n",
    "    'padding': (32, 32),    # 検出ウィンドウの周りのパディング\n",
    "    'scale': 1.05,          # 画像ピラミッドのスケール\n",
    "    'hitThreshold': 0,      # 検出閾値\n",
    "    'finalThreshold': 5     # 重複検出のフィルタリング閾値\n",
    "}\n",
    "\n",
    "# 画像の読み込み\n",
    "img = cv2.imread(\"img/img01.jpg\")\n",
    "\n",
    "# グレースケールに変換（HOG検出器は通常グレースケール画像を使用）\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# 人物検出の実行\n",
    "# human: 検出された人物の座標 (x, y, width, height)\n",
    "# r: 信頼度スコア（この例では使用していません）\n",
    "human, r = hog.detectMultiScale(gray, **hogParams)\n",
    "\n",
    "# 検出結果の処理\n",
    "if (len(human) > 0):  # 人物が検出された場合\n",
    "    for (x, y, w, h) in human:\n",
    "        # 検出された人物を白い矩形で囲む\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), (255,255,255), 3)\n",
    "\n",
    "# 結果を画像ファイルとして保存\n",
    "cv2.imwrite(\"temp.jpg\", img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 画像内の人の顔を検出する\n",
    "\n",
    "OpenCVライブラリを使用して画像内の顔を検出し、検出された顔の周りに赤い矩形を描画します。具体的には、Haar Cascade分類器を用いて顔検出を行い、結果を視覚化して新しい画像ファイルとして保存します。\n",
    "\n",
    "主な処理の流れは以下の通りです：\n",
    "1. 顔検出用の分類器をロード\n",
    "2. 画像ファイルを読み込みグレースケールに変換\n",
    "3. 顔検出を実行\n",
    "4. 検出された顔の位置に矩形を描画\n",
    "5. 結果を新しい画像ファイルとして保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCVライブラリをインポート\n",
    "import cv2\n",
    "\n",
    "# 顔検出のための準備\n",
    "# Haar Cascade分類器の XMLファイルを指定\n",
    "cascade_file = \"haarcascade_frontalface_alt.xml\"\n",
    "# 分類器をロード\n",
    "cascade = cv2.CascadeClassifier(cascade_file)\n",
    "\n",
    "# 画像の読み込みと顔検出\n",
    "# 画像ファイルを読み込む\n",
    "img = cv2.imread(\"img/img02.jpg\")\n",
    "# グレースケールに変換（顔検出はグレースケール画像で行う）\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "# 顔検出を実行（最小サイズを50x50ピクセルに設定）\n",
    "face_list = cascade.detectMultiScale(gray, minSize=(50, 50))\n",
    "\n",
    "# 検出した顔に矩形を描画\n",
    "for (x, y, w, h) in face_list:\n",
    "    color = (0, 0, 225)  # 赤色 (BGR形式)\n",
    "    pen_w = 3  # 線の太さ\n",
    "    # 顔の位置に矩形を描画\n",
    "    cv2.rectangle(img, (x, y), (x+w, y+h), color, thickness=pen_w)\n",
    "\n",
    "# 結果を画像ファイルとして保存\n",
    "cv2.imwrite(\"temp.jpg\", img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 画像内の人がどこに顔を向けているのかを検出する\n",
    "\n",
    "OpenCVとdlibライブラリを使用して、画像内の顔を検出し、顔のランドマーク（特徴点）を識別し、さらに顔の向きを推定します。顔認識システム、感情分析、AR（拡張現実）アプリケーションなどに活用できます。\n",
    "\n",
    "主な機能は以下の通りです：\n",
    "1. 顔検出：画像内の顔を検出し、矩形で囲みます。\n",
    "2. ランドマーク検出：各顔に対して68個の特徴点（ランドマーク）を検出します。\n",
    "3. 重心計算：顔の輪郭と内部のランドマークそれぞれの重心を計算します。\n",
    "4. 顔の方向推定：2つの重心の相対位置から顔の左右の向きを推定します。\n",
    "5. 視覚化：検出結果を元の画像上に描画し、新しい画像ファイルとして保存します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリをインポート\n",
    "import cv2\n",
    "import dlib\n",
    "import math\n",
    "\n",
    "# 顔のランドマーク検出器と顔検出器の準備\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "# 画像の読み込みと顔検出\n",
    "img = cv2.imread(\"img/img02.jpg\")\n",
    "dets = detector(img, 1)  # 1はアップサンプリング回数\n",
    "\n",
    "# 検出された各顔に対して処理\n",
    "for k, d in enumerate(dets):\n",
    "    # 顔のランドマークを検出\n",
    "    shape = predictor(img, d)\n",
    "    \n",
    "    # 描画用の色とパラメータを設定\n",
    "    color_f = (0, 0, 225)  # 顔領域の色 (赤)\n",
    "    color_l_out = (255, 0, 0)  # 外側のランドマークの色 (青)\n",
    "    color_l_in = (0, 255, 0)  # 内側のランドマークの色 (緑)\n",
    "    line_w = 3  # 線の太さ\n",
    "    circle_r = 3  # 円の半径\n",
    "    fontType = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    fontSize = 1\n",
    "\n",
    "    # 顔領域を矩形で囲み、顔番号を表示\n",
    "    cv2.rectangle(img, (d.left(), d.top()), (d.right(), d.bottom()), color_f, line_w)\n",
    "    cv2.putText(img, str(k), (d.left(), d.top()), fontType, fontSize, color_f, line_w)\n",
    "\n",
    "    # 重心計算の準備\n",
    "    num_of_points_out = 17  # 輪郭を構成するランドマークの数\n",
    "    num_of_points_in = shape.num_parts - num_of_points_out  # 内側のランドマークの数\n",
    "    gx_out = gy_out = gx_in = gy_in = 0  # 重心座標の初期化\n",
    "\n",
    "    # 各ランドマークに対して処理\n",
    "    for shape_point_count in range(shape.num_parts):\n",
    "        shape_point = shape.part(shape_point_count)\n",
    "        \n",
    "        # ランドマークの描画と重心の計算\n",
    "        if shape_point_count < num_of_points_out:\n",
    "            cv2.circle(img, (shape_point.x, shape_point.y), circle_r, color_l_out, line_w)\n",
    "            gx_out += shape_point.x / num_of_points_out\n",
    "            gy_out += shape_point.y / num_of_points_out\n",
    "        else:\n",
    "            cv2.circle(img, (shape_point.x, shape_point.y), circle_r, color_l_in, line_w)\n",
    "            gx_in += shape_point.x / num_of_points_in\n",
    "            gy_in += shape_point.y / num_of_points_in\n",
    "\n",
    "    # 重心位置を描画\n",
    "    cv2.circle(img, (int(gx_out), int(gy_out)), circle_r, (0,0,255), line_w)  # 外側の重心 (赤)\n",
    "    cv2.circle(img, (int(gx_in), int(gy_in)), circle_r, (0,0,0), line_w)  # 内側の重心 (黒)\n",
    "\n",
    "    # 顔の方向を計算\n",
    "    theta = math.asin(2*(gx_in-gx_out)/(d.right()-d.left()))\n",
    "    radian = theta * 180 / math.pi\n",
    "    print(f\"顔方位:{theta} (角度:{radian}度)\")\n",
    "\n",
    "    # 顔の方向を画像に表示\n",
    "    textPrefix = \"   left \" if radian < 0 else \"   right \"\n",
    "    textShow = f\"{textPrefix}{abs(radian):.1f} deg.\"\n",
    "    cv2.putText(img, textShow, (d.left(), d.top()), fontType, fontSize, color_f, line_w)\n",
    "\n",
    "# 処理結果の画像を保存\n",
    "cv2.imwrite(\"temp.jpg\", img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webカメラから読み込んだ映像データから顔を検出する\n",
    "\n",
    "Webカメラからのリアルタイム映像で顔検出、顔のランドマーク（特徴点）識別、および顔の向き推定を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import math\n",
    "\n",
    "# 顔のランドマーク検出器と顔検出器の準備\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "# Webカメラの映像キャプチャを開始\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "\n",
    "# Webカメラからの映像を連続的に処理\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()  # フレームを取得\n",
    "    if ret:\n",
    "        # グレースケールに変換（顔検出の精度向上のため）\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # 顔検出を実行\n",
    "        dets = detector(gray, 1)  # 1はアップサンプリング回数\n",
    "        \n",
    "        # 検出された各顔に対して処理\n",
    "        for k, d in enumerate(dets):\n",
    "            # 顔のランドマークを検出\n",
    "            shape = predictor(frame, d)\n",
    "            \n",
    "            # 描画用のパラメータ設定\n",
    "            color_f = (0, 0, 225)  # 顔領域の色 (赤)\n",
    "            color_l_out = (255, 0, 0)  # 外側のランドマークの色 (青)\n",
    "            color_l_in = (0, 255, 0)  # 内側のランドマークの色 (緑)\n",
    "            line_w = 2  # 線の太さ\n",
    "            circle_r = 2  # 円の半径\n",
    "            fontType = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            fontSize = 1\n",
    "\n",
    "            # 顔領域を矩形で囲み、顔番号を表示\n",
    "            cv2.rectangle(frame, (d.left(), d.top()), (d.right(), d.bottom()), color_f, line_w)\n",
    "            cv2.putText(frame, str(k), (d.left(), d.top()), fontType, fontSize, color_f, line_w)\n",
    "\n",
    "            # 重心計算の準備\n",
    "            num_of_points_out = 17  # 輪郭を構成するランドマークの数\n",
    "            num_of_points_in = shape.num_parts - num_of_points_out  # 内側のランドマークの数\n",
    "            gx_out = gy_out = gx_in = gy_in = 0  # 重心座標の初期化\n",
    "\n",
    "            # 各ランドマークに対して処理\n",
    "            for shape_point_count in range(shape.num_parts):\n",
    "                shape_point = shape.part(shape_point_count)\n",
    "                # ランドマークの描画と重心の計算\n",
    "                if shape_point_count < num_of_points_out:\n",
    "                    cv2.circle(frame, (shape_point.x, shape_point.y), circle_r, color_l_out, line_w)\n",
    "                    gx_out += shape_point.x / num_of_points_out\n",
    "                    gy_out += shape_point.y / num_of_points_out\n",
    "                else:\n",
    "                    cv2.circle(frame, (shape_point.x, shape_point.y), circle_r, color_l_in, line_w)\n",
    "                    gx_in += shape_point.x / num_of_points_in\n",
    "                    gy_in += shape_point.y / num_of_points_in\n",
    "\n",
    "            # 重心位置を描画\n",
    "            cv2.circle(frame, (int(gx_out), int(gy_out)), circle_r, (0, 0, 255), line_w)  # 外側の重心 (赤)\n",
    "            cv2.circle(frame, (int(gx_in), int(gy_in)), circle_r, (0, 0, 0), line_w)  # 内側の重心 (黒)\n",
    "\n",
    "            # 顔の方向を計算\n",
    "            theta = math.asin(2 * (gx_in - gx_out) / (d.right() - d.left()))\n",
    "            radian = theta * 180 / math.pi\n",
    "\n",
    "            # 顔の方向を画像に表示\n",
    "            textPrefix = \"   left \" if radian < 0 else \"   right \"\n",
    "            textShow = f\"{textPrefix}{abs(radian):.1f} deg.\"\n",
    "            cv2.putText(frame, textShow, (d.left(), d.top()), fontType, fontSize, color_f, line_w)\n",
    "        \n",
    "        # 処理結果をリアルタイムで表示\n",
    "        cv2.imshow(\"Webcam\", frame)\n",
    "    \n",
    "    # 'q'キーが押されたらループを終了\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# リソースを解放してウィンドウを閉じる\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 顔認識 (データベース無)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "\n",
    "# dlibのモデルのロード\n",
    "face_detector = dlib.get_frontal_face_detector()  # 顔検出用\n",
    "shape_predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")  # 顔ランドマーク用\n",
    "face_rec_model = dlib.face_recognition_model_v1(\"dlib_face_recognition_resnet_model_v1.dat\")  # 顔認識用\n",
    "\n",
    "# 既知の顔のデータ（ここでは事前に特徴ベクトルを用意）\n",
    "# 顔の特徴ベクトルを格納するリスト (既知の顔)\n",
    "known_face_encodings = []\n",
    "known_face_names = []\n",
    "\n",
    "# Webカメラ映像のキャプチャ\n",
    "# cap = cv2.VideoCapture(0)\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "\n",
    "# フレーム処理\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        # グレースケールに変換（検出精度向上のため）\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # 顔検出\n",
    "        faces = face_detector(gray, 1)\n",
    "        \n",
    "        for face in faces:\n",
    "            # 顔ランドマークを取得\n",
    "            shape = shape_predictor(frame, face)\n",
    "            \n",
    "            # 顔の特徴ベクトルを計算\n",
    "            face_descriptor = face_rec_model.compute_face_descriptor(frame, shape)\n",
    "            face_encoding = np.array(face_descriptor)\n",
    "            \n",
    "            # 既知の顔と照合（簡易的に距離で比較）\n",
    "            matches = []\n",
    "            for known_encoding in known_face_encodings:\n",
    "                dist = np.linalg.norm(known_encoding - face_encoding)\n",
    "                matches.append(dist)\n",
    "            \n",
    "            # 閾値以下の距離なら顔が一致とみなす（例: 0.6以下）\n",
    "            if matches and min(matches) < 0.6:\n",
    "                match_index = matches.index(min(matches))\n",
    "                name = known_face_names[match_index]\n",
    "            else:\n",
    "                name = \"Unknown\"\n",
    "            \n",
    "            # 顔領域の表示\n",
    "            cv2.rectangle(frame, (face.left(), face.top()), (face.right(), face.bottom()), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, name, (face.left(), face.top() - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "        \n",
    "        # 結果を表示\n",
    "        cv2.imshow(\"Webcam\", frame)\n",
    "    \n",
    "    # 'q'キーが押されたら終了\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# リソースを解放してウィンドウを閉じる\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 検出した情報を統合し，タイムラプスを作る\n",
    "\n",
    "入力動画からタイムラプス動画を生成し、同時に人物検出を行います。タイムラプス動画とは、長時間にわたって撮影された一連の写真や映像を、通常よりも速いペースで再生することで作成される映像技法です。この技法により、通常では目に見えないほどゆっくりとした変化や動きを、短時間で観察できるようになります。\n",
    "\n",
    "主な機能は以下の通りです：\n",
    "1. 動画読み込み：指定された動画ファイルを読み込みます。\n",
    "2. タイムラプス生成：10フレームごとに1フレームを抽出し、新しい動画を作成します。\n",
    "3. 人物検出：HOG（Histogram of Oriented Gradients）ディスクリプタを使用して、各フレームで人物を検出します。\n",
    "4. 可視化：検出された人物の周りに白い矩形を描画します。\n",
    "5. 出力：処理されたフレームを新しいタイムラプス動画として保存します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "print(\"タイムラプス生成を開始します\")\n",
    "\n",
    "# 動画ファイルの読み込み\n",
    "# cap = cv2.VideoCapture(\"mov/mov01.avi\")  # 別の動画ファイルを使用する場合はこちらのコメントを解除\n",
    "cap = cv2.VideoCapture(\"vtest.avi\")\n",
    "\n",
    "# 動画の基本情報を取得\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# HOG（Histogram of Oriented Gradients）人物検出器の設定\n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "hogParams = {\n",
    "    'winStride': (8, 8),    # 検出ウィンドウの移動ステップ\n",
    "    'padding': (32, 32),    # 検出ウィンドウの周りのパディング\n",
    "    'scale': 1.05,          # 画像ピラミッドのスケール\n",
    "    'hitThreshold': 0,      # 検出閾値\n",
    "    'finalThreshold': 5     # 重複検出のフィルタリング閾値\n",
    "}\n",
    "\n",
    "# タイムラプス動画の設定\n",
    "movie_name = \"timelapse.avi\"\n",
    "fourcc = cv2.VideoWriter_fourcc('X', 'V', 'I', 'D')  # コーデックの設定\n",
    "video = cv2.VideoWriter(movie_name, fourcc, 30, (width, height))  # 出力動画の設定\n",
    "\n",
    "num = 0  # フレームカウンター\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()  # フレームを1つ読み込む\n",
    "    if ret:\n",
    "        if (num % 10 == 0):  # 10フレームごとに処理（タイムラプス効果）\n",
    "            # グレースケールに変換（人物検出の精度向上のため）\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # 人物検出を実行\n",
    "            human, r = hog.detectMultiScale(gray, **hogParams)\n",
    "            \n",
    "            if (len(human) > 0):  # 人物が検出された場合\n",
    "                for (x, y, w, h) in human:\n",
    "                    # 検出された人物を白い矩形で囲む\n",
    "                    cv2.rectangle(frame, (x, y), (x + w, y + h), (255,255,255), 3)\n",
    "            \n",
    "            # 処理したフレームをタイムラプス動画に追加\n",
    "            video.write(frame)\n",
    "    else:\n",
    "        break  # 動画の終わりに達したらループを終了\n",
    "    \n",
    "    num += 1  # フレームカウンターを増加\n",
    "\n",
    "# リソースの解放\n",
    "video.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"タイムラプス生成を終了しました\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 到達度確認"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "課題1：画像の読み込みと表示\n",
    "* Google画像検索で任意の画像ファイルをダウンロードしてOpenCVを用いて読み込み表示してみよう\n",
    "\n",
    "課題2：画像内の人物検出\n",
    "* Webカメラから取得した画像内の人物数をカウントして、画面に表示してみよう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 応用課題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 顔のプライバシー保護（顔のモザイク処理）\n",
    "    * 目的：画像内の顔を検出し、検出した顔にモザイク処理を施してプライバシーを保護する。\n",
    "* 顔向きによるユーザーの注意状態の検出\n",
    "    * 目的：Webカメラからの映像で顔の向きを検出し、ユーザーが画面から目をそらした回数をカウントする。\n",
    "* 動体検知によるセキュリティカメラの実装\n",
    "    * 目的：Webカメラからの映像で動体を検知し、動きがあったフレームを画像として保存する。\n",
    "* 人物出現頻度のヒートマップ作成\n",
    "    * 目的：動画内で人物がよく現れる位置をヒートマップとして可視化する。\n",
    "* 笑顔検出による自動写真撮影\n",
    "    * 目的：ユーザーが笑顔になったときに自動的に写真を撮影するプログラムを作成する。\n",
    "* 顔認識による出席管理システム\n",
    "    * 目的：顔認識を用いて個人を識別し、出席情報を記録するシステムを構築する。\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
